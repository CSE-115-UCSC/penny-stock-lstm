{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1bdabce-182c-4f58-9d04-ebb4536dcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import os, csv\n",
    "import sys\n",
    "from time import time, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "983fffc4-7e88-488d-b9d8-a52b74599f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQlite connected with historicaldata.db\n"
     ]
    }
   ],
   "source": [
    "# Connect to SQlite database\n",
    "def connect_db(database_name_with_path):\n",
    "    cursor = None\n",
    "    try:\n",
    "      sqliteConnection = sqlite3.connect(database_name_with_path)\n",
    "      cursor = sqliteConnection.cursor()\n",
    "      print(f'SQlite connected with {db}')\n",
    "    except:\n",
    "      sys.stderr.write(\"Failed to connect to database\")\n",
    "    return cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a175b9ab-f8af-4a55-9535-ac799a1f0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(296751,)]\n"
     ]
    }
   ],
   "source": [
    "# Test connection query\n",
    "try:\n",
    "  query = \"SELECT COUNT(*) FROM all_historical;\"\n",
    "  cursor.execute(query)\n",
    "  print(cursor.fetchall())\n",
    "\n",
    "except:\n",
    "  sys.stderr.write(\"Failed to execute query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da80e1c8-db5c-4e53-b7bf-d29296b41c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling threshold percentile value - if in a day, this much price raise occurs on day's starting price, then label buy \n",
    "gain_threshold = 20\n",
    "row_data_threshold = 25\n",
    "#csv filename\n",
    "csv_filename = \"min_day_rows-\" + (str)(row_data_threshold) + \"-min_price_gain-\" + (str)(gain_threshold) + \"pc-\" + (str)(time()) + \".csv\"\n",
    "uncompressed_csv_filename = \"uc-\" + csv_filename\n",
    "\n",
    "# setup database connection\n",
    "cursor = connect_db('historicaldata.db')\n",
    "if (cursor == None):\n",
    "    sys.stderr.write(\"Failed to get cursor...\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d581125f-c912-4b3a-865e-9094b01c9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared successfully and loaded in variable: dataset. A csv file (min_day_rows-25-min_price_gain-20pc-1721711727.7495308.csv) is also created in same directory.\n",
      "An uncompressed version of the dataset is stored in csv file uc-min_day_rows-25-min_price_gain-20pc-1721711727.7495308.csv in the same directory.\n"
     ]
    }
   ],
   "source": [
    "# let's create the dataset\n",
    "all_historical_dataset = list()\n",
    "tickers_list = list()\n",
    "ticker_dates_list = list()\n",
    "ticker_date_data_list = list()\n",
    "\n",
    "dataset = list()\n",
    "uncompressed_dataset = list()\n",
    "#try:\n",
    "query = \"select distinct(ticker) as ticker from all_historical where 1 order by ticker;\"\n",
    "cursor.execute(query)\n",
    "tickers_list = [list(i) for i in cursor.fetchall()]\n",
    "ticker_data = list()\n",
    "for ticker in tickers_list:\n",
    "    \n",
    "    ticker_query = \"select distinct(strftime('%Y-%m-%d', time/1000, 'unixepoch')) as ticker_date from all_historical where ticker='\" + ticker[0] + \"' order by ticker_date;\"\n",
    "    cursor.execute(ticker_query)\n",
    "    ticker_dates_list = [list(j) for j in cursor.fetchall()]\n",
    "\n",
    "    date_data = list()\n",
    "    for ticker_date in ticker_dates_list:\n",
    "        \n",
    "        ticker_date_query = \"select strftime('%H:%M', time/1000, 'unixepoch') as ticker_time, volume, volume_weighted_average, time from all_historical where ticker='\" + ticker[0] + \"' and strftime('%Y-%m-%d', time/1000, 'unixepoch') = '\" + ticker_date[0] + \"' order by ticker_time;\" \n",
    "        cursor.execute(ticker_date_query)\n",
    "        ticker_date_data_list = [list(k) for k in cursor.fetchall()]\n",
    "\n",
    "        first_hm_price = 0.0 # actually it's 0\n",
    "        previous_hm = (np.nan, np.nan, np.nan, np.nan) # actually it's 0\n",
    "        row_data = list()\n",
    "        row_label = list()\n",
    "        label = \"no_buy\"\n",
    "        f = 0\n",
    "        is_first_hm_price_set = False\n",
    "        for ticker_date_data_row in ticker_date_data_list:\n",
    "            # Let's skip the 0 hours data\n",
    "            f = f + 1\n",
    "            if ((ticker_date_data_row[0]) == '00:00'):\n",
    "                continue\n",
    "            \n",
    "            # if (first_hm_price == 0. and first_hm_price < (float)(ticker_date_data_list[f][2])):\n",
    "            if (not(is_first_hm_price_set)):\n",
    "                first_hm_price = (float)(ticker_date_data_list[f][2])\n",
    "                is_first_hm_price_set = True\n",
    "            \n",
    "            # If previous time is more than 5 seconds but not 0000 hours, we loop to impute values by previous rows\n",
    "            if (previous_hm != (np.nan, np.nan, np.nan, np.nan)):\n",
    "                time_diff = (int)((ticker_date_data_row[3] - previous_hm[3])/(1000 * 60 * 5))\n",
    "                if time_diff > 1:\n",
    "                    for td in range (time_diff-1):\n",
    "                        previous_hm_0_plus_5 = ((int)(previous_hm[0])+5)\n",
    "                        if (previous_hm_0_plus_5%100==60):\n",
    "                            previous_hm_0_plus_5 = (int)(previous_hm[0])+45 #(+100-55)\n",
    "                        row_data.append(((previous_hm_0_plus_5), (float)(previous_hm[1]), (float)(previous_hm[2])))\n",
    "                        row_label.append((label))  ### Since, this is just copying previous rows, retaining price raise label\n",
    "                        previous_hm = ((previous_hm_0_plus_5), (float)(previous_hm[1]), (float)(previous_hm[2]), (previous_hm[3]))\n",
    "                        # time_diff = time_diff - 1\n",
    "                        uncompressed_dataset.append((ticker[0], ticker_date[0], (previous_hm_0_plus_5), (float)(previous_hm[1]), (float)(previous_hm[2]), label))\n",
    "\n",
    "                # parallely, we label the row as buy if there's a 30% raise in price, default=no_buy\n",
    "                \n",
    "                price_change_percentage = float((float(ticker_date_data_row[2] - first_hm_price) * 100)/float(first_hm_price))\n",
    "                if price_change_percentage >= gain_threshold:  # price gain/raise\n",
    "                    label = \"buy\"\n",
    "\n",
    "            ticker_date_data_row_0 = ticker_date_data_row[0].replace(\":\",\"\")\n",
    "            row_data.append(((int)(ticker_date_data_row_0), (float)(ticker_date_data_row[1]), (float)(ticker_date_data_row[2])))\n",
    "            row_label.append((label))\n",
    "            previous_hm = ((int)(ticker_date_data_row_0), (float)(ticker_date_data_row[1]), (float)(ticker_date_data_row[2]),(ticker_date_data_row[3]))\n",
    "            uncompressed_dataset.append((ticker[0], ticker_date[0], (int)(ticker_date_data_row_0), (float)(ticker_date_data_row[1]), (float)(ticker_date_data_row[2]), label))\n",
    "        #print(row_data)\n",
    "        #exit(1)\n",
    "        # avoid adding data that has zero records or less than 60% from a day (25.2/42 records, starting at 4:30 ending at 8:00)\n",
    "        if (len(row_data) > row_data_threshold):\n",
    "            date_data.append((ticker[0], ticker_date[0], row_data, row_label))\n",
    "\n",
    "            with open(csv_filename, 'a', newline='') as file:\n",
    "                csvwriter = csv.writer(file)\n",
    "                csvwriter.writerow((ticker[0], ticker_date[0], row_data, row_label))\n",
    "            file.close()\n",
    "    ticker_data.append((date_data))\n",
    "#print(ticker_data)\n",
    "dataset = ticker_data\n",
    "#print(dataset)\n",
    "\n",
    "print(\"Dataset prepared successfully and loaded in variable: dataset. A csv file (\" + csv_filename + \") is also created in same directory.\")\n",
    "\n",
    "with open(uncompressed_csv_filename, 'a', newline='') as file:\n",
    "    csvwriter = csv.writer(file)\n",
    "    for uncompressed_datarow in uncompressed_dataset:\n",
    "        csvwriter.writerow(uncompressed_datarow)\n",
    "file.close()\n",
    "\n",
    "print(\"An uncompressed version of the dataset is stored in csv file \" + uncompressed_csv_filename + \" in the same directory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa7c8a1-6303-4ecb-9921-a622fe5cb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e50696-3927-4f7e-8a5a-48b55f418d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SQL QUERIES ###\n",
    "'''\n",
    "select ticker, strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch') as ticker_date, strftime('%H:%M', \"time\"/1000, 'unixepoch') as ticker_time, volume_weighted_average from CDAK where 1 order by ticker, ticker_date, ticker_time;\n",
    "\n",
    "select ticker , strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch') as ticker_date from all_historical where 1 group by ticker order by ticker, ticker_date;\n",
    "\n",
    "select ticker , max(strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch')) as max_ticker_date, min(strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch')) as min_ticker_date from all_historical where 1 group by ticker order by ticker;\n",
    "\n",
    "select ticker , max(strftime('%H:%M', \"time\"/1000, 'unixepoch')) as max_ticker_time, min(strftime('%H:%M', \"time\"/1000, 'unixepoch')) as min_ticker_time, avg(strftime('%H:%M', \"time\"/1000, 'unixepoch')) as avg_ticker_time from all_historical where 1 group by ticker order by ticker;\n",
    "\n",
    "select ticker , strftime('%H:%M', \"time\"/1000, 'unixepoch') as ticker_time, count(strftime('%H:%M', \"time\"/1000, 'unixepoch')) as count_ticker_time from all_historical where 1 group by ticker, ticker_time order by ticker, ticker_time;\n",
    "\n",
    "\n",
    "select distinct(ticker) as ticker from all_historical where 1 order by ticker;\n",
    "\n",
    "select distinct(strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch')) as ticker_date from all_historical where ticker=\"AAU\" order by ticker_date;\n",
    "\n",
    "select time, strftime('%Y-%m-%d', \"time\"/1000, 'unixepoch') as ticker_date, strftime('%H:%M', time/1000, 'unixepoch') as ticker_time, volume, volume_weighted_average from all_historical where ticker=\"AAU\" order by ticker_time, ticker_date;\n",
    "\n",
    "select ticker, strftime('%Y-%m-%d %H:%M:%S', time/1000, 'unixepoch') as ticker_date from all_historical where time>=1671494400000 and time<=1671557700000 ;\n",
    "\n",
    "select ticker, strftime('%Y-%m-%d %H:%M:%S', time/1000, 'unixepoch') as ticker_date, strftime('%H:%M', time/1000, 'unixepoch') as ticker_time, volume, volume_weighted_average from all_historical where strftime('%Y-%m-%d', time/1000, 'unixepoch') between '2023-11-12' and '2023-11-25' order by ticker, ticker_date;\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df246f9f-d17c-4644-8648-93c8920149f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0289f8-645d-440f-b0b7-a5e0b5037c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
