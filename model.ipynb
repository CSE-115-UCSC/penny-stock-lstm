{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidanjmaldonado/penny-stock-lstm/blob/main/penny_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AB9VHvt_xNXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import sqlite3\n",
        "import requests\n",
        "import sys\n",
        "# from library.DataSetProcessor import DataSetProcessor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# constants\n",
        "SEQUENCE_LENGTH = 78 # 1 day long\n",
        "PREDICTION_LENGTH = 78 # 1 day long\n",
        "NUM_FEATURES = 3 # close, volume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJBjdbHzoWvZ"
      },
      "source": [
        "# Create database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DGRIE9XnRw8",
        "outputId": "c53b5aec-8331-490b-b5b8-37e403b087c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request to download database succeeded\n"
          ]
        }
      ],
      "source": [
        "# download database from github repository\n",
        "historical_url = \"https://raw.githubusercontent.com/CSE-115-UCSC/penny-stock-lstm/main/historicaldata.db\"\n",
        "scrape_request = requests.get(historical_url)\n",
        "\n",
        "try:\n",
        "  # contingent on request status\n",
        "  scrape_request = requests.get(historical_url)\n",
        "  scrape_request.raise_for_status()\n",
        "\n",
        "  # create local database from pull, name 'historicaldata.db'\n",
        "  with open(\"historical.db\", \"wb\") as db_file:\n",
        "    db_file.write(scrape_request.content)\n",
        "    \n",
        "  print(\"Request to download database succeeded\")\n",
        "\n",
        "\n",
        "except:\n",
        "  # report failed request status\n",
        "  sys.stderr.write(\"Request to download database failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KApKP8mve16y",
        "outputId": "eac13848-bd1a-4869-8752-8d266eaab896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQlite connected with historical.db\n"
          ]
        }
      ],
      "source": [
        "# connect to SQlite database\n",
        "try:\n",
        "    db = 'historical.db'\n",
        "    sqliteConnection = sqlite3.connect(db)\n",
        "    cursor = sqliteConnection.cursor()\n",
        "    print(f'SQlite connected with {db}')\n",
        "\n",
        "except:\n",
        "    # report failed request status\n",
        "    sys.stderr.write(\"Failed to connect to database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or4POn4Ye_Lw",
        "outputId": "a222f695-d83c-4f79-bc32-348cb0749353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success querying all historical\n"
          ]
        }
      ],
      "source": [
        "# query all historical stock data from Database\n",
        "try:\n",
        "    query = f\"SELECT * FROM all_historical;\"\n",
        "    cursor.execute(query)\n",
        "    if cursor.fetchone() is None:\n",
        "        raise Exception(\"No results\")\n",
        "\n",
        "    print(f\"Success querying all historical\")\n",
        "    # turn SQlite Database into Pandas Dataframe\n",
        "    data = pd.read_sql_query(query, sqliteConnection)\n",
        "\n",
        "except:\n",
        "    sys.stderr.write(f\"Failed to select all historical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_MxXjzzSQ1b_"
      },
      "outputs": [],
      "source": [
        "# using the column 'time' (millisecond) add a new column 'dates' with datetime\n",
        "dates = pd.to_datetime(data['time'], unit='ms')\n",
        "dates = dates.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
        "dates = dates.dt.tz_localize(None)\n",
        "\n",
        "# tickers array for checking sequence quality\n",
        "tickers = data['ticker']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalized features (close, volume) table for training\n",
        "normalized_data = pd.DataFrame(columns=['volume_weighted_average','volume', 'number_of_trades'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>ticker</th>\n",
              "      <th>volume</th>\n",
              "      <th>volume_weighted_average</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>time</th>\n",
              "      <th>number_of_trades</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACHR</td>\n",
              "      <td>269.0</td>\n",
              "      <td>3.0307</td>\n",
              "      <td>3.0301</td>\n",
              "      <td>3.030</td>\n",
              "      <td>3.0301</td>\n",
              "      <td>3.0300</td>\n",
              "      <td>1657643400000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACHR</td>\n",
              "      <td>2037.0</td>\n",
              "      <td>3.0362</td>\n",
              "      <td>3.0400</td>\n",
              "      <td>3.040</td>\n",
              "      <td>3.0400</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>1657643700000</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACHR</td>\n",
              "      <td>796.0</td>\n",
              "      <td>3.0364</td>\n",
              "      <td>3.0400</td>\n",
              "      <td>3.035</td>\n",
              "      <td>3.0400</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>1657644000000</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACHR</td>\n",
              "      <td>1795.0</td>\n",
              "      <td>3.0356</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>3.035</td>\n",
              "      <td>3.0400</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>1657644300000</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACHR</td>\n",
              "      <td>10397.0</td>\n",
              "      <td>3.0286</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>3.025</td>\n",
              "      <td>3.0350</td>\n",
              "      <td>3.0212</td>\n",
              "      <td>1657644600000</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id ticker   volume  volume_weighted_average    open  close    high     low  \\\n",
              "0   1   ACHR    269.0                   3.0307  3.0301  3.030  3.0301  3.0300   \n",
              "1   2   ACHR   2037.0                   3.0362  3.0400  3.040  3.0400  3.0350   \n",
              "2   3   ACHR    796.0                   3.0364  3.0400  3.035  3.0400  3.0350   \n",
              "3   4   ACHR   1795.0                   3.0356  3.0350  3.035  3.0400  3.0350   \n",
              "4   5   ACHR  10397.0                   3.0286  3.0350  3.025  3.0350  3.0212   \n",
              "\n",
              "            time  number_of_trades  \n",
              "0  1657643400000                 4  \n",
              "1  1657643700000                35  \n",
              "2  1657644000000                17  \n",
              "3  1657644300000                19  \n",
              "4  1657644600000                87  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dictionary where each key is a stock ticker and the value is the table containing normalized data\n",
        "data_by_ticker = {}\n",
        "for ticker in data['ticker'].unique():\n",
        "    data_by_ticker[ticker] = data[data['ticker'] == ticker].copy()\n",
        "    data_by_ticker[ticker]['normalized_volume_weighted_average'] = data_by_ticker[ticker]['volume_weighted_average'] / data_by_ticker[ticker]['volume_weighted_average'].max() #Normalized closing price data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/s4/t5945xms4x52fkpsd0m4v9340000gn/T/ipykernel_25681/777330838.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  normalized_data = pd.concat([normalized_data, temp_df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "for key in data_by_ticker:\n",
        "    # create a temporary DataFrame to hold the current data\n",
        "    temp_df = pd.DataFrame({\n",
        "        'volume_weighted_average': data_by_ticker[key]['normalized_volume_weighted_average'],\n",
        "        'volume': data_by_ticker[key]['volume'],\n",
        "        'number_of_trades': data_by_ticker[key]['number_of_trades']\n",
        "    })\n",
        "    \n",
        "    # concatenate the temporary DataFrame to the normalized_data DataFrame\n",
        "    normalized_data = pd.concat([normalized_data, temp_df], ignore_index=True)\n",
        "\n",
        "# optionally, you can reset the index if needed\n",
        "normalized_data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>volume_weighted_average</th>\n",
              "      <th>volume</th>\n",
              "      <th>number_of_trades</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.389675</td>\n",
              "      <td>269.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.390383</td>\n",
              "      <td>2037.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.390408</td>\n",
              "      <td>796.0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.390305</td>\n",
              "      <td>1795.0</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.389405</td>\n",
              "      <td>10397.0</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   volume_weighted_average   volume number_of_trades\n",
              "0                 0.389675    269.0                4\n",
              "1                 0.390383   2037.0               35\n",
              "2                 0.390408    796.0               17\n",
              "3                 0.390305   1795.0               19\n",
              "4                 0.389405  10397.0               87"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalized_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MRzML9poTXB"
      },
      "source": [
        "# Train on all historical stock data, sequenced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToUrsdh6ki-s",
        "outputId": "9d50990f-8603-4c63-eb66-d56d2a54b4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid days: 294\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"Generate arrays filled with one-day-long sequences from the normalized dataset\n",
        "\n",
        "Arguments:\n",
        "    - data: stock dataset with 2 columns:\n",
        "        - close prices normalized (0, 1) \n",
        "        - volume\n",
        "    - dates: array of every milisecond timestamp converted to dateTime objects\n",
        "    - tickers: array of every timestamp's corresponding ticker\n",
        "\n",
        "Returns:\n",
        "    - x: array of one-day-long sequences of the normalized dataset for training\n",
        "    - y: array of one-day-long seuqences of the immediate day after for predicting\n",
        "\n",
        "\"\"\"\n",
        "def create_sequences(data, dates, tickers):\n",
        "    \n",
        "    # stores sequences to be returned\n",
        "    xs, ys = [], []\n",
        "    # index refers to the start of a day, therefore start of a 'sequence'\n",
        "    index = 0 \n",
        "     # keeps track of the number of valid sequences for debugging purposes\n",
        "    count = 0\n",
        "\n",
        "    # loop until the end of database, stopping 2 days in advance to make room for last 'context' day and it's corresponding 'prediction' day\n",
        "    while index < len(data) - SEQUENCE_LENGTH - PREDICTION_LENGTH + 1:\n",
        "\n",
        "        # Check if sequence is within a single day (day start == day end) and (ticker start == ticker end)\n",
        "        if dates[index].date() == dates[index + SEQUENCE_LENGTH].date() and tickers[index] == tickers[index + SEQUENCE_LENGTH]:\n",
        "\n",
        "            # append current day (index -> index+SEQ) to x, and append next day (index + SEQ -> index + SEQ + PRE) to y\n",
        "            xs.append(data.iloc[index:index + SEQUENCE_LENGTH])  # Use past data for features\n",
        "            ys.append(data.iloc[index + SEQUENCE_LENGTH:index + SEQUENCE_LENGTH + PREDICTION_LENGTH, 0])  # Only predict 'close' prices\n",
        "\n",
        "            # move index to start of the next day\n",
        "            index += SEQUENCE_LENGTH\n",
        "            count += 1\n",
        "        \n",
        "        # move index to the start of the next \n",
        "        else: # Note: This is the discarding section, can be modified to be \"imputed\" via extending the last known close value until end of day.\n",
        "\n",
        "            # move new_index to the start of the next day\n",
        "            new_index = index\n",
        "            while dates[new_index].date() == dates[new_index + 1].date():\n",
        "                new_index += 1\n",
        "            new_index += 1\n",
        "            \n",
        "            # once new_index reaches next morning, set index to match\n",
        "            index = new_index\n",
        "\n",
        "    # print the number of valid days found for debugging purposes, return arrays of sequences            \n",
        "    print(\"Valid days:\", count)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# create sequences from normalized data\n",
        "x, y = create_sequences(normalized_data, dates, tickers) #Creating the input and grouth truth data from create_sequences function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aidanmaldonado/Documents/stock-prediction/venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.1917 - val_loss: 0.0389\n",
            "Epoch 2/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1463 - val_loss: 0.0366\n",
            "Epoch 3/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1321 - val_loss: 0.0414\n",
            "Epoch 4/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0984 - val_loss: 0.0440\n",
            "Epoch 5/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0803 - val_loss: 0.0446\n",
            "Epoch 6/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0779 - val_loss: 0.0375\n",
            "Epoch 7/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0670 - val_loss: 0.0548\n",
            "Epoch 8/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0601 - val_loss: 0.0431\n",
            "Epoch 9/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0625 - val_loss: 0.0494\n",
            "Epoch 10/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0622 - val_loss: 0.0452\n",
            "Epoch 11/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0641 - val_loss: 0.0385\n",
            "Epoch 12/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0528 - val_loss: 0.0524\n",
            "Epoch 13/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0598 - val_loss: 0.0392\n",
            "Epoch 14/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0518 - val_loss: 0.0504\n",
            "Epoch 15/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0585 - val_loss: 0.0408\n",
            "Epoch 16/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0645 - val_loss: 0.0423\n",
            "Epoch 17/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0553 - val_loss: 0.0413\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x31b1db310>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Split data into 80% / 20% training and testing groups\n",
        "train_size = int(len(x) * 0.8)\n",
        "x_train, x_test = x[:train_size], x[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Type adjustment string -> float\n",
        "x_train = x_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)))\n",
        "# Need both layers because return_sequences will send its output to another LSTM layer which is required before sending to Dense layer \n",
        "model.add(LSTM(50))\n",
        " # Makes readable by NN, NN doesn't predict on sequences so it needs single dimension values\n",
        "model.add(Dense(40, activation='relu'))\n",
        "# Prevents overfitting\n",
        "model.add(Dropout(0.1))\n",
        "# Takes the results from the last LSTM layer and predicts the stock prices for PREDICTION_LENGTH steps ahead\n",
        "model.add(Dense(PREDICTION_LENGTH)) \n",
        "#Compiles the model with an adam optimizer and a mean squared error loss function\n",
        "model.compile(optimizer='adam', loss='mse') \n",
        "\n",
        "# Train the model with early stopping to prevent over fitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the model weights to an external file\n",
        "model.save('model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
